"""
RSS feed collector - handles all RSS-based sources.
"""

import asyncio
import re
from datetime import datetime, timezone
from typing import Optional
import aiohttp
import feedparser
from .base import BaseCollector, NewsItem


class RSSCollector(BaseCollector):
    """Collect news from RSS feeds."""

    def __init__(self, source_id: str, source_config: dict):
        super().__init__(source_config)
        self.source_id = source_id
        self.feed_url = source_config["url"]
        self.source_name = source_config["name"]
        self.category = source_config.get("category", "general")
        self.keywords = source_config.get("keywords", [])
        self.max_items = source_config.get("max_items", 10)

    async def collect(self) -> list[NewsItem]:
        """Fetch and parse RSS feed."""
        if not self.is_enabled():
            return []

        try:
            # Use a browser-like User-Agent to avoid being blocked (e.g. by 36Kr)
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
            }
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    self.feed_url,
                    timeout=aiohttp.ClientTimeout(total=30, connect=10),
                    headers=headers,
                    allow_redirects=True
                ) as response:
                    if response.status != 200:
                        print(f"[{self.source_name}] HTTP {response.status}")
                        return []

                    try:
                        content = await response.text()
                    except aiohttp.ClientPayloadError:
                        # Fallback for partial payloads - some servers are buggy
                        content = await response.read()
                        content = content.decode('utf-8', errors='replace')
        except Exception as e:
            print(f"[{self.source_name}] Fetch error: {type(e).__name__}: {e}")
            return []

        # Parse feed
        feed = feedparser.parse(content)
        items = []

        for entry in feed.entries[:self.max_items * 2]:  # Fetch extra for filtering
            title = entry.get("title", "")

            # 优先使用 content (通常包含完整文章), 其次是 summary/description
            content_list = entry.get("content", [])
            full_content = ""
            if content_list:
                # 寻找 text/html 或 text/plain
                for c in content_list:
                    if c.get("type") in ["text/html", "text/plain"]:
                        full_content = c.get("value", "")
                        break

            # Check if content is invalid (anti-bot)
            if self._is_invalid_content(full_content):
                # Fallback to summary if content is invalid
                full_content = ""

            # 如果 content 为空 (或无效)，尝试使用 summary_detail 或 summary
            if not full_content:
                full_content = entry.get("summary", entry.get("description", ""))

            # Clean HTML tags for filtering and display
            clean_content = self._clean_html(full_content)

            # Filter out invalid content (anti-bot responses) - Final check
            if self._is_invalid_content(clean_content):
                print(f"[{self.source_name}] Skipped invalid content: {title}")
                continue

            # Apply keyword filter
            # Check title and content combination
            if not self.filter_by_keywords(f"{title} {clean_content}", self.keywords):
                continue

            # Parse publish date
            published = self._parse_date(entry)

            # Extract image URL
            image_url = self._extract_image(entry, full_content)

            item = NewsItem(
                title=title,
                url=entry.get("link", ""),
                source=self.source_name,
                category=self.category,
                published=published,
                summary=clean_content[:1000],  # 保留更多内容给 LLM 总结
                content=clean_content,         # 保存完整内容
                author=entry.get("author"),
                tags=[tag.term for tag in entry.get("tags", [])][:5],
                image_url=image_url,
            )
            items.append(item)

            if len(items) >= self.max_items:
                break

        print(f"[{self.source_name}] Collected {len(items)} items")
        return items

    def _extract_image(self, entry, summary: str) -> Optional[str]:
        """从RSS条目中提取图片URL"""
        # 方法1: media:content 或 media:thumbnail
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if media.get('type', '').startswith('image/') or media.get('medium') == 'image':
                    return media.get('url')

        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            return entry.media_thumbnail[0].get('url')

        # 方法2: enclosure
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enc in entry.enclosures:
                if enc.get('type', '').startswith('image/'):
                    return enc.get('href') or enc.get('url')

        # 方法3: 从 content 中提取 <img> 标签
        content = entry.get('content', [{}])[0].get('value', '') if entry.get('content') else ''
        full_text = summary + content

        img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', full_text)
        if img_match:
            img_url = img_match.group(1)
            # 过滤掉太小的图片（通常是图标）
            if not any(x in img_url.lower() for x in ['icon', 'logo', 'avatar', '1x1', 'pixel']):
                return img_url

        # 方法4: image 字段
        if hasattr(entry, 'image') and entry.image:
            if isinstance(entry.image, dict):
                return entry.image.get('href') or entry.image.get('url')
            elif isinstance(entry.image, str):
                return entry.image

        return None

    def _parse_date(self, entry) -> Optional[datetime]:
        """Parse date from feed entry."""
        for date_field in ["published_parsed", "updated_parsed", "created_parsed"]:
            time_struct = entry.get(date_field)
            if time_struct:
                try:
                    return datetime(*time_struct[:6], tzinfo=timezone.utc)
                except:
                    pass
        return None

    def _clean_html(self, text: str) -> str:
        """Remove HTML tags from text but preserve some structure."""
        if not text:
            return ""

        # Replace block elements and breaks with newlines to preserve structure
        text = re.sub(r'<(p|div|br|li|h[1-6]|tr)[^>]*>', '\n', text, flags=re.IGNORECASE)

        # Remove all other tags
        text = re.sub(r'<[^>]+>', '', text)

        # Collapse multiple spaces but preserve newlines
        lines = []
        for line in text.split('\n'):
            cleaned_line = re.sub(r'\s+', ' ', line).strip()
            if cleaned_line:
                lines.append(cleaned_line)

        return '\n'.join(lines)


    def _is_invalid_content(self, text: str) -> bool:
        """Check if content is an anti-bot response or invalid."""
        if not text:
            return False
        invalid_markers = [
            "request result",
            "enable javascript",
            "javascript is disabled",
            "please enable js",
            "access denied",
            "security check"
        ]
        text_lower = text.lower()
        return any(marker in text_lower for marker in invalid_markers)


async def collect_all_rss(rss_config: dict) -> list[NewsItem]:
    """Collect from all configured RSS sources."""
    collectors = []

    for source_id, source_config in rss_config.items():
        if source_config.get("enabled", True):
            collectors.append(RSSCollector(source_id, source_config))

    # Run all collectors concurrently
    tasks = [c.collect() for c in collectors]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    all_items = []
    for result in results:
        if isinstance(result, list):
            all_items.extend(result)
        elif isinstance(result, Exception):
            print(f"Collector error: {result}")

    return all_items
