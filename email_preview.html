<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Digest - 2026å¹´01æœˆ22æ—¥</title>
    <style>
        /* Reset and base styles */
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
            color: #1a1a1a;
            background-color: #f5f5f5;
            margin: 0;
            padding: 0;
        }

        .container {
            max-width: 680px;
            margin: 0 auto;
            background-color: #ffffff;
        }

        /* Header */
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 32px 24px;
            text-align: center;
        }

        .header h1 {
            margin: 0 0 8px 0;
            font-size: 28px;
            font-weight: 700;
            letter-spacing: -0.5px;
        }

        .header .date {
            opacity: 0.9;
            font-size: 14px;
        }

        /* Highlights section */
        .highlights {
            background: linear-gradient(180deg, #f8f9ff 0%, #ffffff 100%);
            padding: 28px 24px;
            border-bottom: 2px solid #e5e7eb;
        }

        .highlights h2 {
            color: #4f46e5;
            font-size: 18px;
            margin: 0 0 20px 0;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .highlights-content {
            font-size: 15px;
            color: #374151;
        }

        /* è¦ç‚¹åˆ—è¡¨æ ·å¼ */
        .highlight-item {
            background: #ffffff;
            border-left: 4px solid #4f46e5;
            padding: 16px 20px;
            margin-bottom: 16px;
            border-radius: 0 8px 8px 0;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        }

        .highlight-item:last-child {
            margin-bottom: 0;
        }

        .highlight-number {
            display: inline-block;
            background: #4f46e5;
            color: white;
            width: 24px;
            height: 24px;
            border-radius: 50%;
            text-align: center;
            line-height: 24px;
            font-size: 13px;
            font-weight: 600;
            margin-right: 10px;
        }

        .highlight-text {
            display: inline;
            line-height: 1.6;
        }

        /* Category sections */
        .category {
            padding: 28px 24px;
            border-bottom: 2px solid #e5e7eb;
        }

        .category:last-child {
            border-bottom: none;
        }

        .category-header {
            font-size: 18px;
            font-weight: 600;
            margin: 0 0 20px 0;
            color: #1f2937;
            display: flex;
            align-items: center;
            gap: 8px;
            padding-bottom: 12px;
            border-bottom: 1px solid #f3f4f6;
        }

        .category-header .count {
            background: #e5e7eb;
            color: #6b7280;
            font-size: 12px;
            padding: 2px 8px;
            border-radius: 12px;
            font-weight: 500;
        }

        /* News items - å¡ç‰‡å¼å¸ƒå±€ */
        .news-item {
            background: #fafafa;
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 20px;
            border: 1px solid #e5e7eb;
            transition: box-shadow 0.2s;
        }

        .news-item:last-child {
            margin-bottom: 0;
        }

        /* å›¾æ–‡å¸ƒå±€ */
        .news-content-wrapper {
            display: flex;
            gap: 16px;
        }

        .news-text {
            flex: 1;
        }

        .news-image {
            width: 120px;
            height: 80px;
            border-radius: 8px;
            object-fit: cover;
            flex-shrink: 0;
        }

        .news-title {
            margin: 0 0 10px 0;
            font-size: 16px;
            font-weight: 600;
            line-height: 1.5;
        }

        .news-title a {
            color: #1f2937;
            text-decoration: none;
        }

        .news-title a:hover {
            color: #4f46e5;
        }

        .news-meta {
            font-size: 13px;
            color: #6b7280;
            margin-bottom: 12px;
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            align-items: center;
        }

        .news-source {
            color: #4f46e5;
            font-weight: 500;
        }

        .news-summary {
            font-size: 14px;
            color: #4b5563;
            margin: 12px 0 0 0;
            line-height: 1.7;
        }

        /* ç¿»è¯‘æ ‡è®° */
        .translated {
            color: #059669;
            font-size: 11px;
            padding: 2px 6px;
            background: #ecfdf5;
            border-radius: 4px;
        }

        /* æœºæ„æ ‡ç­¾ */
        .org-tag {
            display: inline-block;
            background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
            color: white;
            font-size: 11px;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 500;
        }

        /* Tags */
        .tags {
            margin-top: 12px;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
        }

        .tag {
            background: #e0e7ff;
            color: #4338ca;
            font-size: 11px;
            padding: 3px 10px;
            border-radius: 12px;
            font-weight: 500;
        }

        /* Paper specific styling */
        .paper-item {
            background: linear-gradient(135deg, #f0fdf4 0%, #fafafa 100%);
            border: 1px solid #bbf7d0;
        }

        .paper-item .news-title a {
            color: #059669;
        }

        .paper-item .news-title a:hover {
            color: #047857;
        }

        .paper-authors {
            font-size: 12px;
            color: #6b7280;
            font-style: italic;
            margin-top: 4px;
        }

        .paper-org-tag {
            display: inline-block;
            background: linear-gradient(135deg, #059669 0%, #047857 100%);
            color: white;
            font-size: 11px;
            padding: 3px 10px;
            border-radius: 4px;
            font-weight: 600;
            margin-right: 8px;
        }

        /* Footer */
        .footer {
            background: #f9fafb;
            padding: 24px;
            text-align: center;
            font-size: 13px;
            color: #6b7280;
        }

        .footer a {
            color: #4f46e5;
            text-decoration: none;
        }

        /* Responsive */
        @media (max-width: 600px) {
            .header {
                padding: 24px 16px;
            }
            .header h1 {
                font-size: 24px;
            }
            .category, .highlights {
                padding: 20px 16px;
            }
            .news-item {
                padding: 16px;
            }
            .news-content-wrapper {
                flex-direction: column-reverse;
            }
            .news-image {
                width: 100%;
                height: 160px;
            }
            .highlight-item {
                padding: 12px 16px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Header -->
        <div class="header">
            <h1>ğŸ¤– AI Daily Digest</h1>
            <div class="date">2026å¹´01æœˆ22æ—¥ Â· 19 æ¡èµ„è®¯</div>
        </div>

        <!-- Highlights -->
        
        <div class="highlights">
            <h2>âš¡ ä»Šæ—¥è¦ç‚¹</h2>
            <div class="highlights-content">
                1. TechCrunch æŠ¥é“ Motional å°†åœ¨2025å¹´äºæ‹‰æ–¯ç»´åŠ æ–¯æ¨å‡ºæ— äººé©¾é©¶ robotaxi æœåŠ¡ï¼Œé‡å¿ƒè½¬å‘ AI é©±åŠ¨çš„æŠ€æœ¯æ¶æ„
2. Google é’ˆå¯¹ç‰¹å®šåŒ»ç–—æŸ¥è¯¢ç§»é™¤äº† AI Overviews åŠŸèƒ½ï¼Œæ­¤å‰è¢«æ›å‡ºæä¾›è¯¯å¯¼æ€§å¥åº·ä¿¡æ¯
3. arXiv æœ€æ–°è®ºæ–‡èšç„¦å›¾ç¥ç»ç½‘ç»œè®­ç»ƒå’Œå¤§æ¨¡å‹é›†æˆè§£ç æŠ€æœ¯
4. 36æ°ªï¼šæ™ºè°±å•æ—¥æ¶¨å¹…è¶…31%ï¼Œå›½å†…å¤§æ¨¡å‹æ¦‚å¿µè‚¡æ´»è·ƒ
5. å°åº¦ä¿¡å®å·¥ä¸šå®£å¸ƒ776äº¿ç¾å…ƒæŠ•èµ„è®¡åˆ’ï¼Œå°†å»ºè®¾å°åº¦æœ€å¤§ AI æ•°æ®ä¸­å¿ƒ
            </div>
        </div>
        

        <!-- Categories -->
        
        <div class="category">
            <h2 class="category-header">
                ğŸ’° è¡Œä¸šæŠ•èèµ„
                <span class="count">5</span>
            </h2>

            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://techcrunch.com/2026/01/22/quadric-rides-the-shift-from-cloud-ai-to-on-device-inference-and-its-paying-off/" target="_blank">Quadric rides the shift from cloud AI to on-device inference â€” and itâ€™s paying off</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">TechCrunch AI</span>
                            
                            <span class="news-date">01-22 12:00</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">Quadric aims to help companies and governments build programmable on-device AI chips that can run fast-changing models locally.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">Startups</span>
                            
                            <span class="tag">AI</span>
                            
                            <span class="tag">Exclusive</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://techcrunch.com/2026/01/22/former-google-trio-is-building-an-interactive-ai-powered-learning-app-for-kids/" target="_blank">Former Google trio is building an interactive AI-powered learning app for kids</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">TechCrunch AI</span>
                            
                            <span class="news-date">01-22 11:00</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">Big tech companies and upcoming startups want to use generative AI to build software and hardware for kids. A lot of those experiences are limited to text or voice, and kids might not find that captivating. Three former Google employees want to get over that hurdle with their generative AI-powered interactive app, Sparkli. Sparkli was [&#8230;]</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">AI</span>
                            
                            <span class="tag">Apps</span>
                            
                            <span class="tag">Google</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://techcrunch.com/2026/01/21/not-to-be-outdone-by-openai-apple-is-reportedly-developing-an-ai-wearable/" target="_blank">Not to be outdone by OpenAI, Apple is reportedly developing an AI wearable</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">TechCrunch AI</span>
                            
                            <span class="news-date">01-22 00:20</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">Should this wearable materialize, it could be released as early as 2027, according to a report on the device.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">AI</span>
                            
                            <span class="tag">Hardware</span>
                            
                            <span class="tag">Ai Pin</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://techcrunch.com/2026/01/21/sources-project-sglang-spins-out-as-radixark-with-400m-valuation-as-inference-market-explodes/" target="_blank">Sources: Project SGLang spins out as RadixArk with $400M valuation as inference market explodes</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">TechCrunch AI</span>
                            
                            <span class="news-date">01-21 23:24</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">SGLang, which originated as an open source research project at Ion Stoicaâ€™s UC Berkeley lab, has raised capital from Accel.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">AI</span>
                            
                            <span class="tag">Venture</span>
                            
                            <span class="tag">Accel</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://techcrunch.com/2026/01/21/a-timeline-of-the-u-s-semiconductor-market-in-2025/" target="_blank">A timeline of the US semiconductor market in 2025</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">TechCrunch AI</span>
                            
                            <span class="news-date">01-21 22:46</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">From leadership changes at legacy semiconductor companies to wishy washy policy around chip exports, a lot happened last year.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">AI</span>
                            
                            <span class="tag">Enterprise</span>
                            
                            <span class="tag">Hardware</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
        </div>
        
        <div class="category">
            <h2 class="category-header">
                ğŸ‡¨ğŸ‡³ å›½å†…åŠ¨æ€
                <span class="count">5</span>
            </h2>

            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://36kr.com/p/3650396043092101?f=rss" target="_blank">æ°ªæ˜Ÿæ™šæŠ¥ï½œæ½˜åŠŸèƒœï¼šä»Šå¹´é™å‡†é™æ¯è¿˜æœ‰ä¸€å®šçš„ç©ºé—´ï¼›é«˜ç››ä¸Šè°ƒ2026å¹´åº•é‡‘ä»·ç›®æ ‡è‡³5400ç¾å…ƒï¼›ç™¾å·æ¨å‡ºæœ€ä½å¹»è§‰å¾ªè¯å¢å¼ºåŒ»ç–—å¤§æ¨¡å‹M3 Plus</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">36æ°ª AI</span>
                            
                            <span class="news-date">01-22 11:34</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">å¤§å…¬å¸ï¼š
ç©¿è¶Šè€…å…¬å¸å·²é¢„è®¢é¦–æ‰¹20ä½™ä½å¤ªç©ºæ¸¸å®¢ï¼Œé¢„è®¡2028å¹´å®ç°è½½äººé¦–é£
1æœˆ22æ—¥ï¼ŒåŒ—äº¬ç©¿è¶Šè€…è½½äººèˆªå¤©ç§‘æŠ€æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°â€œç©¿è¶Šè€…â€ï¼‰ä¸¾è¡Œâ€œå¤ªç©ºæ—…æ¸¸å…¨çƒå‘å¸ƒä¼šâ€ã€‚ä¼šä¸Šï¼Œé¦–æ‰¹å¤ªç©ºæ¸¸å®¢äº®ç›¸ï¼ŒåŒ…æ‹¬ä¸­å›½å·¥ç¨‹é™¢é™¢å£«æç«‹æµ§ã€æ™ºå…ƒæœºå™¨äººCMOé‚±æ’ã€æ¢è·¯è€…å“ç‰Œåˆ›å§‹äººç‹é™ã€å•Ÿèµ‹èµ„æœ¬è‘£äº‹é•¿å‚…å“²å®½ç­‰ï¼Œæ­¤å¤–ï¼Œé¦–æ‰¹å¤ªç©ºæ¸¸å®¢è¿˜åŒ…æ‹¬ä¸€åç¡…åŸºç”Ÿç‰©ï¼Œä¸ºä¼—æ“æœºå™¨äººPM01ã€‚å›½å†…é¦–è‰˜å•†ä¸šè½½äººé£èˆ¹â€œç©¿è¶Šè€…å£¹å·ï¼ˆCYZ1ï¼‰â€å…¨å°ºå¯¸è¯•éªŒèˆ±é¦–æ¬¡å…¬å¼€å±•ç¤ºã€‚â€œç©¿è¶Šè€…å£¹å·ï¼ˆCYZ1ï¼‰â€æŠ€æœ¯å›¢é˜Ÿä¼šå‰æ¥å—è¯åˆ¸æ—¶æŠ¥è®°è€…é‡‡è®¿æ—¶è¡¨ç¤ºï¼Œç›®å‰å·²é¢„è®¢äº†è¶…ä¸‰è‰˜èˆ¹ï¼Œåˆè®¡20ä½™ä½å¤ªç©ºæ¸¸å®¢ï¼Œé¢„è®¡2028å¹´å°†å®ç°è½½äººé¦–é£ã€‚ï¼ˆè¯åˆ¸æ—¶æŠ¥ï¼‰
æœˆä¹‹æš—é¢æ€»è£å¼ äºˆå½¤ï¼šKimiä»…ä½¿ç”¨ç¾å›½é¡¶å°–å®éªŒå®¤1%çš„èµ„æºï¼Œæœ€æ–°æ¨¡å‹å°†å¾ˆå¿«å‘å¸ƒ
1æœˆ21æ—¥ï¼Œæœˆä¹‹æš—é¢Kimiæ€»è£å¼ äºˆå½¤å‡ºå¸­åœ¨ç‘å£«è¾¾æ²ƒæ–¯ä¸¾è¡Œçš„ä¸–ç•Œç»æµè®ºå›2026å¹´å¹´ä¼šæ—¶è¡¨ç¤ºï¼ŒKimiä»…ä½¿ç”¨ç¾å›½é¡¶å°–å®éªŒå®¤1%çš„èµ„æºï¼Œå°±å¼€æ”¾å‡ºKimi K2ã€Kimi K2 Thinkingè¿™æ ·å…¨çƒé¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼Œç”šè‡³åœ¨éƒ¨åˆ†æ€§èƒ½ä¸Šè¶…è¶Šç¾å›½çš„é¡¶å°–é—­æºæ¨¡å‹ã€‚å¼ äºˆå½¤é€éœ²ï¼ŒKimiæŠ•å…¥å¤§é‡ç²¾åŠ›å°†å·¥ç¨‹åŒ–æ€ç»´å¼•å…¥ç ”ç©¶ç¯èŠ‚ï¼Œç¡®ä¿æ‰€æœ‰ç®—æ³•åˆ›æ–°éƒ½èƒ½åœ¨ç”Ÿäº§ç³»ç»Ÿä¸­å¤§è§„æ¨¡ç¨³å®šè¿è¡Œï¼Œæ®å¥¹é€éœ²ï¼ŒKimiæœ€æ–°æ¨¡å‹å°†å¾ˆå¿«å‘å¸ƒã€‚ï¼ˆæ¾æ¹ƒï¼‰
æ½˜åŠŸèƒœï¼šä»Šå¹´é™å‡†é™æ¯è¿˜æœ‰ä¸€å®šçš„ç©ºé—´
ä¸­å›½äººæ°‘é“¶è¡Œè¡Œé•¿æ½˜åŠŸèƒœè¡¨ç¤ºï¼Œ2026å¹´ï¼Œä¸­å›½äººæ°‘é“¶è¡Œå°†ç»§ç»­å®æ–½å¥½é€‚åº¦å®½æ¾çš„è´§å¸æ”¿ç­–ï¼ŒæŠŠä¿ƒè¿›ç»æµç¨³å®šå¢é•¿ã€ç‰©ä»·åˆç†å›å‡ä½œä¸ºè´§å¸æ”¿ç­–çš„é‡è¦è€ƒé‡ï¼Œå‘æŒ¥å¢é‡æ”¿ç­–å’Œå­˜é‡æ”¿ç­–é›†æˆæ•ˆåº”ï¼Œä¸ºç»æµç¨³å®šå¢é•¿ã€é«˜è´¨é‡å‘å±•å’Œé‡‘èå¸‚åœºç¨³å®šè¿è¡Œè¥é€ è‰¯å¥½çš„è´§å¸é‡‘èç¯å¢ƒï¼Œä¸ºå®ç°â€œåäº”äº”â€è‰¯å¥½å¼€å±€æä¾›æœ‰åŠ›çš„é‡‘èæ”¯æ’‘ã€‚æ€»é‡æ”¿ç­–æ–¹é¢ï¼Œçµæ´»é«˜æ•ˆè¿ç”¨é™å‡†é™æ¯ç­‰å¤šç§è´§å¸æ”¿ç­–å·¥å…·ï¼Œä¿æŒæµåŠ¨æ€§å……è£•ï¼Œä½¿ç¤¾ä¼šèèµ„è§„æ¨¡ã€è´§å¸ä¾›åº”é‡å¢é•¿åŒç»æµå¢é•¿ã€ä»·æ ¼æ€»æ°´å¹³é¢„æœŸç›®æ ‡ç›¸åŒ¹é…ã€‚ä»Šå¹´é™å‡†é™æ¯è¿˜æœ‰ä¸€å®šçš„ç©ºé—´ã€‚äººæ°‘é“¶è¡Œè¿˜å°†åšå¥½åˆ©ç‡æ”¿ç­–æ‰§è¡Œå’Œç›‘ç£ï¼Œä¿ƒè¿›ç¤¾ä¼šç»¼åˆèèµ„æˆæœ¬ä½ä½è¿è¡Œã€‚ï¼ˆæ–°åç¤¾ï¼‰
é«˜ç››ä¸Šè°ƒ2026å¹´åº•é‡‘ä»·ç›®æ ‡è‡³5400ç¾å…ƒ
é«˜ç››åœ¨å‘¨ä¸‰å‘å¸ƒçš„æœ€æ–°ç ”æŠ¥ä¸­ï¼Œå°†2026å¹´12æœˆçš„é»„é‡‘ç›®æ ‡ä»·ä»å…ˆå‰çš„4900ç¾å…ƒ/ç›å¸ä¸Šè°ƒè‡³5400ç¾å…ƒ/ç›å¸ã€‚é«˜ç››è¡¨ç¤ºï¼Œæ­¤æ¬¡è°ƒä»·çš„æ ¸å¿ƒé€»è¾‘åœ¨äºç§äººéƒ¨é—¨ä¸ºå¯¹å†²å…¨çƒå®è§‚ä¸æ”¿ç­–ä¸ç¡®å®šæ€§è€Œè¿›è¡Œçš„é»„é‡‘å¤šå…ƒåŒ–é…ç½®æ­£åœ¨å…‘ç°ï¼Œä¸”ç›¸å…³â€œå¯¹å†²æŒä»“â€åœ¨2026å¹´å¯èƒ½ä¿æŒç¨³å®šã€‚ï¼ˆç¬¬ä¸€è´¢ç»ï¼‰
ASMPTå®£å¸ƒå‰¥ç¦»æ——ä¸‹SMTä¸šåŠ¡
36æ°ªè·æ‚‰ï¼Œ1æœˆ22æ—¥ï¼Œå…¨çƒåŠ</p>
                        
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://www.jiqizhixin.com/articles/2026-01-22-14" target="_blank">å¹»è§‰ç‡ä¸åˆ°3%ï¼Œç‹å°å·æŠŠåŒ»ç”Ÿç‰ˆçš„DeepSeekå…è´¹äº†</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">æœºå™¨ä¹‹å¿ƒ</span>
                            
                            <span class="news-date">01-22 11:17</span>
                            
                            
                        </div>
                        
                        
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://www.jiqizhixin.com/articles/2026-01-22-13" target="_blank">æ¸…åå§šç­æ ¡å‹åˆ˜å£®å›¢é˜Ÿå†å‘åŠ›ï¼Œæ— éœ€å½’ä¸€åŒ–çš„Transformeræ€§èƒ½è¿›åŒ–</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">æœºå™¨ä¹‹å¿ƒ</span>
                            
                            <span class="news-date">01-22 11:13</span>
                            
                            
                        </div>
                        
                        
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://www.jiqizhixin.com/articles/2026-01-22-12" target="_blank">è‹¹æœå…¥å±€AI Pinï¼Œæˆ–å¯¹æ ‡OpenAIï¼Œèƒ½å¦æ‰“ç ´ã€Œç”µå­åƒåœ¾ã€é­”å’’ï¼Ÿ</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">æœºå™¨ä¹‹å¿ƒ</span>
                            
                            <span class="news-date">01-22 11:06</span>
                            
                            
                        </div>
                        
                        
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://36kr.com/p/3650412256731265?f=rss" target="_blank">é˜¿é‡Œå¹³å¤´å“¥å¯åŠ¨ä¸Šå¸‚è®¡åˆ’ï¼Œå·²å¸ƒå±€å…¨æ ˆAIèŠ¯ç‰‡</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">36æ°ª AI</span>
                            
                            <span class="news-date">01-22 10:37</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">1æœˆ22æ—¥ï¼Œæ¥è¿‘å¸‚åœºäººå£«ç§°ï¼Œé˜¿é‡Œå·´å·´é›†å›¢å·²å†³å®šæ”¯æŒæ——ä¸‹èŠ¯ç‰‡å…¬å¸å¹³å¤´å“¥æœªæ¥ç‹¬ç«‹ä¸Šå¸‚ã€‚å¹³å¤´å“¥æ˜¯é˜¿é‡Œå·´å·´å…¨èµ„å­å…¬å¸ï¼Œäº2018å¹´æˆç«‹ã€‚
æˆç«‹8å¹´ä»¥æ¥ï¼Œå…¬å¸å·²åœ¨è®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œç­‰é¢†åŸŸæ¨å‡ºå¤šæ¬¾æ€§èƒ½ä¸šç•Œé¡¶çº§çš„èŠ¯ç‰‡ï¼Œæ®æ‚‰ï¼Œå¹³å¤´å“¥è‡ªç ”PPUå·²æˆä¸ºä¸­å›½æ–°å¢AIç®—åŠ›å¸‚åœºçš„ä¸»åŠ›èŠ¯ç‰‡ä¹‹ä¸€ã€‚
ä¸€ã€äº’è”ç½‘å…¬å¸é‡Œï¼Œè‡ªç ”èŠ¯ç‰‡çš„æ—©æœŸæ¢ç´¢è€…
èŠ¯ç‰‡ä¸€ç›´è¢«è®¤ä¸ºæ˜¯ç§‘æŠ€é¢†åŸŸæœ€éš¾å‘çš„éª¨å¤´ä¹‹ä¸€ï¼Œè€Œå¹³å¤´å“¥åˆ›ç«‹ä¹‹å‰ï¼Œé˜¿é‡Œå·´å·´çš„ç¡¬ä»¶ç ”å‘ã€Œå±¥å†ã€å‡ ä¹ç©ºç™½ï¼Œäº’è”ç½‘å…¬å¸é€ èŠ¯åœ¨å›½å†…æ›´æ— å…ˆä¾‹ï¼Œå¹³å¤´å“¥ç ”å‘èŠ¯ç‰‡çš„éš¾åº¦ä¸äºšäºé˜¿é‡ŒæŠ•å…¥äº‘è®¡ç®—ã€‚
è™½ç„¶ç°å®å¾ˆéª¨æ„Ÿï¼Œä½†åœ¨è¿™åœºæŠ€æœ¯å’Œèµ„æœ¬çš„æ¸¸æˆé‡Œï¼Œé˜¿é‡Œçš„æˆ˜ç•¥å†³å¿ƒä¸äºšäºå¯¹é˜¿é‡Œäº‘çš„é•¿æœŸæŠ•å…¥ã€‚
å…¬å¸æˆç«‹åï¼Œå†…éƒ¨èŠ¯ç‰‡ç ”å‘çš„è·¯çº¿å’Œæ¨¡å¼éå¸¸æ¸…æ™°ï¼Œå³é‡ç‚¹å›´ç»•äº‘ç«¯æ•°æ®ä¸­å¿ƒåœºæ™¯å¼€å±•ä¸€ç³»åˆ—äº§å“çº¿ï¼Œè¿™ä¸å½“æ—¶ç‹¬ç«‹èŠ¯ç‰‡å…¬å¸ç ”å‘æ ‡å“èŠ¯ç‰‡çš„æ¨¡å¼å¤§ç›¸å¾„åº­ï¼Œä½†è¿™ä¸€æ¨¡å¼ä¹Ÿè®©å¹³å¤´å“¥æˆä¸ºèŠ¯ç‰‡äº§ä¸šé“¾é‡Œæœ€å¤§çš„é»‘é©¬ã€‚
ä¼ ç»ŸåŠå¯¼ä½“å‚å•†ç ”å‘èŠ¯ç‰‡ï¼Œéœ€è¦å…ˆè°ƒç ”å®¢æˆ·ã€æ”¶é›†éœ€æ±‚ï¼Œç„¶åæ‰è¿›å…¥æ¼«é•¿çš„èŠ¯ç‰‡å®šä¹‰å’Œè®¾è®¡é˜¶æ®µï¼ŒèŠ¯ç‰‡ç ”å‘çš„å‰æœŸéœ€è¦æ¶ˆè€—å¤§é‡æ—¶é—´ã€‚è€Œå¹³å¤´å“¥èƒŒé é˜¿é‡Œäº‘ï¼Œèƒ½æ›´å¥½åœ°ç†è§£å®¢æˆ·åœºæ™¯ï¼Œä»¥åŠäº‘ç«¯ç®—åŠ›çš„éœ€æ±‚ï¼Œè¿™å¤§å¹…ç¼©çŸ­äº†å¹³å¤´å“¥èŠ¯ç‰‡ä»è®¾è®¡åˆ°åº”ç”¨è½åœ°çš„å‘¨æœŸã€‚
2019å¹´ï¼Œæˆç«‹ä¸€å¹´çš„å¹³å¤´å“¥å°è¯•ç‰›åˆ€ï¼Œæ¨å‡ºäº†æ——ä¸‹ç¬¬ä¸€é¢—èŠ¯ç‰‡â€”â€”AIæ¨ç†èŠ¯ç‰‡å«å…‰800ï¼Œè¿™æ˜¯ä¸€é¢—é’ˆå¯¹åœºæ™¯æ·±åº¦å®šåˆ¶çš„èŠ¯ç‰‡ï¼Œé‡‡ç”¨è‡ªç ”æ¶æ„ï¼Œæ¨ç†æ€§èƒ½è¾¾åˆ°78563IPSï¼Œæ¯ç§’å¯å¤„ç†7ä¸‡8åƒå¼ å›¾ç‰‡ï¼Œåˆ›é€ äº†å½“æ—¶åŒç±»èŠ¯ç‰‡é¢†åŸŸçš„æ€§èƒ½å’Œèƒ½æ•ˆæ¯”çš„ä¸¤é¡¹ç¬¬ä¸€ï¼Œè¯¥èŠ¯ç‰‡é€æ­¥åº”ç”¨äºåŒ11æ·˜å®ä¸»æœåœºæ™¯ã€‚
ä¸æ­¤åŒæ—¶ï¼Œå¹³å¤´å“¥è¿˜å¯åŠ¨äº†éš¾åº¦æ›´é«˜çš„é€šç”¨CPUèŠ¯ç‰‡ç ”å‘ã€‚2021äº‘æ –å¤§ä¼šä¸Šï¼Œå¹³å¤´å“¥å‘å¸ƒé˜¿é‡Œé¦–ä¸ªé€šç”¨æœåŠ¡å™¨èŠ¯ç‰‡å€šå¤©710ï¼Œæ€§èƒ½è¶…è¿‡åŒæœŸä¸šç•Œæ ‡æ†20%ï¼Œèƒ½æ•ˆæ¯”æå‡è¶…50%ã€‚
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šç”¨CPUæ˜¯è®¡ç®—ç³»ç»Ÿçš„å¤§è„‘ï¼Œå®ƒçš„è®¾è®¡æ˜¯åŠå¯¼ä½“è¡Œä¸šæœ€éš¾è·¨è¶Šçš„å‡ åº§å¤§å±±ä¹‹ä¸€ï¼Œæ—¶è‡³ä»Šæ—¥ï¼ŒæŒæ¡è¿™ä¸€æŠ€æœ¯èƒ½åŠ›çš„ä¼ä¸šä¹Ÿå¯¥å¯¥å¯æ•°ï¼Œé˜¿é‡Œæ˜¯ä¸šç•Œé¦–å®¶å…·å¤‡CPUç ”å‘è®¾è®¡èƒ½åŠ›çš„ä¸­å›½äº’è”ç½‘å…¬å¸ã€‚
ç›®å‰ï¼Œå€šå¤©710å·²é€šè¿‡é˜¿é‡Œäº‘æœåŠ¡å¤§è§„æ¨¡åº”ç”¨äºè§†é¢‘ç¼–è§£ç ã€é«˜æ€§èƒ½è®¡ç®—ã€æ¸¸æˆç­‰é¢†åŸŸï¼›å¤§æ´‹å½¼å²¸ï¼Œäºšé©¬é€ŠåŸºäºè‡ªç ”GravitonèŠ¯ç‰‡çš„EC2å®ä¾‹ä¹Ÿå·²æˆä¸ºAWSä¸»åŠ›CPUç®—åŠ›ã€‚é˜¿é‡Œä»¥åŠäºšé©¬é€Šå‡ ä¹åŒä¸€æ—¶é—´è¯æ˜äº†è‡ªç ”èŠ¯ç‰‡ä¸äº‘æœåŠ¡åœºæ™¯æ·±åº¦ååŒçš„é€ èŠ¯æ¨¡å¼è·‘é€šã€‚
å¦‚ä»Šï¼Œè¿™ä¸€æ¨¡å¼å·²é€æ¸è¢«å›½å†…å…¶å®ƒäº’è”ç½‘å¤§å‚æ•ˆä»¿ã€‚
äºŒã€æŠ¼ä¸­AIæµªæ½®ï¼Œå·²é›†é½å…¨æ ˆèŠ¯ç‰‡äº§å“
2020å¹´å‰åï¼Œæ­£å€¼ä¸Šä¸€è½®AIæ·±åº¦å­¦ä¹ æ³¡æ²«ç ´è£‚æœŸï¼Œä¹Ÿæ˜¯æœ¬è½®AI</p>
                        
                        
                    </div>
                    
                    <img class="news-image" src="https://img.36krcdn.com/hsossms/20260122/v2_a6f71083c19e4e8f90fe315c75297dba@1199336245_oswg880769oswg2073oswg1060_img_jpg?x-oss-process=image/quality,q_90/format,jpg/interlace,1" alt="" loading="lazy" onerror="this.style.display='none'">
                    
                </div>
            </div>
            
        </div>
        
        <div class="category">
            <h2 class="category-header">
                ğŸ“„ å‰æ²¿è®ºæ–‡
                <span class="count">5</span>
            </h2>

            
            <div class="news-item paper-item">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <span class="paper-org-tag">Apple</span>
                            
                            <a href="https://arxiv.org/abs/2601.15288v1" target="_blank">APPLE: Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">arXiv</span>
                            
                            <span class="news-date">01-21 18:59</span>
                            
                            
                        </div>
                        
                        <div class="paper-authors">Jiwon Kang, Yeji Choi, JoungBin Lee et al. (9 authors)</div>
                        
                        
                        <p class="news-summary">Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup. However, since real ground truth for face swapping is unavailable, achieving both accurate identity transfer and high-quality attribute preservation remains challenging. In addition, recent diffusion-based approaches attempt to improve visual fidelity through conditional inpainting on masked target images, but the masked condition removes crucial appearance cues of target, resulting in plausible yet misaligned attributes. To address these limitations, we propose APPLE (Attribute-Preserving Pseudo-Labeling), a diffusion-based teacher-student framework that enhances attribute fidelity through attribute-aware pseudo-label supervision. We reformulate face swapping as a conditional deblurring task to more faithfully preserve target-specific attributes such as lighting, skin tone, and makeup. In addition, we introduce an attribute-aware inversion scheme to further improve detailed attribute preservation. Through an elaborate attribute-preserving design for teacher learning, APPLE produces high-quality pseudo triplets that explicitly provide the student with direct face-swapping supervision. Overall, APPLE achieves state-of-the-art performance in terms of attribute preservation and identity transfer, producing more photorealistic and target-faithful results.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">cs.CV</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item paper-item">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <span class="paper-org-tag">Cohere</span>
                            
                            <a href="https://arxiv.org/abs/2601.15284v1" target="_blank">Walk through Paintings: Egocentric World Models from Internet Priors</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">arXiv</span>
                            
                            <span class="news-date">01-21 18:59</span>
                            
                            
                        </div>
                        
                        <div class="paper-authors">Anurag Bagchi, Zhipeng Bao, Homanga Bharadhwaj et al. (6 authors)</div>
                        
                        
                        <p class="news-summary">What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">cs.CV</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item paper-item">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <span class="paper-org-tag">Meta FAIR</span>
                            
                            <a href="https://arxiv.org/abs/2601.15282v1" target="_blank">Rethinking Video Generation Model for the Embodied World</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">arXiv</span>
                            
                            <span class="news-date">01-21 18:59</span>
                            
                            
                        </div>
                        
                        <div class="paper-authors">Yufan Deng, Zilin Pan, Hongyu Zhang et al. (9 authors)</div>
                        
                        
                        <p class="news-summary">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">cs.CV</span>
                            
                            <span class="tag">cs.AI</span>
                            
                            <span class="tag">cs.RO</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item paper-item">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <span class="paper-org-tag">Stability AI</span>
                            
                            <a href="https://arxiv.org/abs/2601.15281v1" target="_blank">StableWorld: Towards Stable and Consistent Long Interactive Video Generation</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">arXiv</span>
                            
                            <span class="news-date">01-21 18:59</span>
                            
                            
                        </div>
                        
                        <div class="paper-authors">Ying Yang, Zhengyao Lv, Tianlin Pan et al. (9 authors)</div>
                        
                        
                        <p class="news-summary">In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">cs.CV</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="news-item paper-item">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <span class="paper-org-tag">Meta FAIR</span>
                            
                            <a href="https://arxiv.org/abs/2601.15267v1" target="_blank">Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">arXiv</span>
                            
                            <span class="news-date">01-21 18:51</span>
                            
                            
                        </div>
                        
                        <div class="paper-authors">Yiran Hu, Huanghai Liu, Chong Wang et al. (18 authors)</div>
                        
                        
                        <p class="news-summary">Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">cs.CY</span>
                            
                            <span class="tag">cs.AI</span>
                            
                            <span class="tag">cs.CL</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
        </div>
        
        <div class="category">
            <h2 class="category-header">
                ğŸ“¬ Newsletterç²¾é€‰
                <span class="count">2</span>
            </h2>

            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://www.a16z.news/p/meet-the-a16z-new-media-fellows" target="_blank">Meet the a16z New Media Fellows</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">a16z AI</span>
                            
                            <span class="news-date">01-21 17:01</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">A few months ago, we announced the a16z New Media Fellowship - an eight-week program and community for operators, builders, and specialists shaping the future of media.
The response was overwhelming: more than 2,000 applications from talented people around the world. Today, we&#8217;re excited to share more about the 65 fellows we&#8217;ve selected to join the inaugural cohort, with a rich array of experience from world-class companies like OpenAI, Google, Apple, Spotify, Vercel, Thrive, General Catalyst, Kalshi, and MrBeast.
A glimpse into the 65 inaugural a16z New Media Fellows
Some people have nicknamed this &#8220;the Thiel Fellowship for the terminally online.&#8221; And true to form, there are a lot of names that you might recognize from your timeline if you are, indeed, terminally online - like Logan Kilpatrick, swyx, Gaby Goldberg, sign&#252;ll, netcapgirl, and Jason Liu.
But it&#8217;s not just the timeline masters. We have top-tier creative talents like Sarah Chieng, Cole Lee</p>
                        
                        
                    </div>
                    
                    <img class="news-image" src="https://substackcdn.com/image/fetch/$s_!rsAb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27ea6e63-b22c-44d6-a3f3-e976213596b8_1080x1350.png" alt="" loading="lazy" onerror="this.style.display='none'">
                    
                </div>
            </div>
            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://www.a16z.news/p/its-time-for-agentic-video-editing" target="_blank">It's time for agentic video editing</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">a16z AI</span>
                            
                            <span class="news-date">01-21 15:29</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">Before we start:
a16z is a remarkable collection of people. Today we&#8217;re highlighting a few of them, as we celebrate some well-deserved promotion and recognition:
We&#8217;re thrilled to share that Alex Immerman has been promoted to General Partner on our Growth fund. You can read Alex&#8217;s most recent piece on a16z news here, on self-driving cars and their lifesaving potential. Congratulations, AI!
Similarly, Matt Bornstein has been promoted to General Partner on our Infra fund. You can read Matt&#8217;s lovingly curated Sci-fi reading list here. Congratulations, Matt!
The whole Infra fund - led by Martin, Raghu, Jennifer and Matt - got a great profile in Bloomberg that you should read.
Subscribe for more from a16z every weekday
And now, on with the show. Here&#8217;s Justine:
2025 was the year of video. AI-generated ads went mainstream. Launch videos from seed stage startups got millions of views. Video podcasts and interviews exploded.
What you didn&#8217;t see was all the w</p>
                        
                        
                    </div>
                    
                    <img class="news-image" src="https://substack-post-media.s3.amazonaws.com/public/images/09b4bad2-f745-4c5c-821a-a5479cbb80c4_2844x1600.png" alt="" loading="lazy" onerror="this.style.display='none'">
                    
                </div>
            </div>
            
        </div>
        
        <div class="category">
            <h2 class="category-header">
                ğŸ™ï¸ æ’­å®¢æ›´æ–°
                <span class="count">1</span>
            </h2>

            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://a16z.simplecast.com/episodes/martin-casado-on-the-demand-forces-behind-ai-pUTZS22R" target="_blank">Martin Casado on the Demand Forces Behind AI</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">a16z Podcast</span>
                            
                            <span class="news-date">01-21 11:00</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">In this feed drop from The Six Five Pod, a16z General Partner Martin Casado discusses how AI is changing infrastructure, software, and enterprise purchasing. He explains why current constraints are driven less by technical limits and more by regulation, particularly around power, data centers, and compute expansion.
The episode also covers how AI is affecting software development, lowering the barrier to coding without eliminating the need for experienced engineers, and how agent-driven tools may shift infrastructure decision-making away from humans.
Watch more from Six Five Media: https://www.youtube.com/@SixFiveMedia
Resources:
Follow Martin Casado on X: https://twitter.com/martin_casado
Follow Patrick Moorhead on X: https://twitter.com/PatrickMoorhead
Follow Daniel Newman on X: https://twitter.com/danielnewmanUV
Stay Updated:
If you enjoyed this episode, be sure to like, subscribe, and share with your friends!
Find a16z on X: https://twitter.com/a16z
Find a16z on LinkedIn: https://w</p>
                        
                        
                        <div class="tags">
                            
                            <span class="tag">saas</span>
                            
                            <span class="tag">llm</span>
                            
                            <span class="tag">llms</span>
                            
                        </div>
                        
                    </div>
                    
                </div>
            </div>
            
        </div>
        
        <div class="category">
            <h2 class="category-header">
                ğŸ¢ å¤§å‚åŠ¨æ€
                <span class="count">1</span>
            </h2>

            
            <div class="news-item ">
                <div class="news-content-wrapper">
                    <div class="news-text">
                        <h3 class="news-title">
                            
                            <a href="https://openai.com/index/higgsfield" target="_blank">How Higgsfield turns simple ideas into cinematic social videos</a>
                            
                        </h3>
                        <div class="news-meta">
                            <span class="news-source">OpenAI Blog</span>
                            
                            <span class="news-date">01-21 10:00</span>
                            
                            
                        </div>
                        
                        
                        <p class="news-summary">Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.</p>
                        
                        
                    </div>
                    
                </div>
            </div>
            
        </div>
        

        <!-- Footer -->
        <div class="footer">
            <p>
                ç”± <strong>AI Daily Digest</strong> è‡ªåŠ¨ç”Ÿæˆ<br>
                æ•°æ®æ¥æºï¼šRSS feeds, arXiv, Hacker News, X/Twitter<br>
                <a href="https://github.com/Rillarilla/ai-daily-digest">æŸ¥çœ‹æºç </a> Â·
                <a href="mailto:rillahai@gmail.com?subject=Feedback">åé¦ˆå»ºè®®</a>
            </p>
        </div>
    </div>
</body>
</html>